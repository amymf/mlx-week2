{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82326\n"
     ]
    }
   ],
   "source": [
    "print(len(ds[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mappings\n",
    "doc_text_to_id = {}\n",
    "doc_id_to_text = {}\n",
    "next_doc_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all documents in the dataset\n",
    "all_documents = []\n",
    "total_passages_count = 0\n",
    "for example in ds[\"train\"]:\n",
    "    passage_texts = example[\"passages\"][\"passage_text\"]\n",
    "    for passage_text in passage_texts:\n",
    "        if passage_text not in doc_text_to_id:  # Add document only if it's not seen before\n",
    "            doc_text_to_id[passage_text] = next_doc_id\n",
    "            doc_id_to_text[next_doc_id] = passage_text\n",
    "            next_doc_id += 1\n",
    "        total_passages_count += 1\n",
    "        all_documents.append(passage_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626907\n",
      "676193\n"
     ]
    }
   ],
   "source": [
    "print(next_doc_id)\n",
    "print(total_passages_count) # includes duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built [query_id, pos_doc_id, neg_doc_id] triplets per batch\n",
    "def build_triplets_batch(batch):\n",
    "    output = {\n",
    "        \"query_id\": [],\n",
    "        \"pos_doc_id\": [],\n",
    "        \"neg_doc_id\": [],\n",
    "    }\n",
    "\n",
    "    for query_id, passage_dict in zip(batch[\"query_id\"], batch[\"passages\"]):\n",
    "        passage_texts = passage_dict[\"passage_text\"]\n",
    "        is_selected_flags = passage_dict[\"is_selected\"]\n",
    "        \n",
    "        # Get a positive doc\n",
    "        pos_doc_id = None\n",
    "        for i, flag in enumerate(is_selected_flags):\n",
    "            if flag:\n",
    "                pos_doc_id = doc_text_to_id.get(passage_texts[i])\n",
    "                break\n",
    "        if pos_doc_id is None:\n",
    "            continue\n",
    "\n",
    "        # Create a set of current query's docs\n",
    "        current_query_docs = set(passage_texts)\n",
    "\n",
    "        # Sample a negative doc efficiently\n",
    "        neg_doc_text = None\n",
    "        attempts = 0\n",
    "        while attempts < 10:  # Try a few times to avoid rare edge cases\n",
    "            candidate = random.choice(all_documents)\n",
    "            if candidate not in current_query_docs:\n",
    "                neg_doc_text = candidate\n",
    "                break\n",
    "            attempts += 1\n",
    "        if neg_doc_text is None:\n",
    "            continue  # fallback if sampling fails\n",
    "\n",
    "        neg_doc_id = doc_text_to_id[neg_doc_text]\n",
    "\n",
    "        output[\"query_id\"].append(query_id)\n",
    "        output[\"pos_doc_id\"].append(pos_doc_id)\n",
    "        output[\"neg_doc_id\"].append(neg_doc_id)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_output = ds[\"train\"].map(\n",
    "    build_triplets_batch,\n",
    "    batched=True,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    desc=\"Building triplets\" # progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_id': 19699, 'pos_doc_id': 5, 'neg_doc_id': 592984}\n",
      "79704\n"
     ]
    }
   ],
   "source": [
    "print(batched_output[0])\n",
    "print(len(batched_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each table\n",
    "queries_df = pd.DataFrame([(query[\"query_id\"], query[\"query\"]) for query in ds[\"train\"]], columns=[\"query_id\", \"query_text\"])\n",
    "documents_df = pd.DataFrame(list(doc_id_to_text.items()), columns=[\"doc_id\", \"doc_text\"])\n",
    "triplets_df = pd.DataFrame(batched_output, columns=[\"query_id\", \"pos_doc_id\", \"neg_doc_id\"])\n",
    "\n",
    "# Save tables\n",
    "queries_df.to_csv(\"queries.tsv\", sep=\"\\t\", index=False)\n",
    "documents_df.to_csv(\"documents.tsv\", sep=\"\\t\", index=False)\n",
    "triplets_df.to_csv(\"triplets.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1862 positive documents that are used by multiple queries\n"
     ]
    }
   ],
   "source": [
    "# Out of curiosity, let's check if there are any positive documents that are used by multiple queries\n",
    "\n",
    "# Group by pos_doc_id and count occurrences\n",
    "pos_doc_counts = triplets_df.groupby('pos_doc_id')['query_id'].count().reset_index()\n",
    "\n",
    "# Filter for positive document IDs that appear more than once\n",
    "duplicate_pos_docs = pos_doc_counts[pos_doc_counts['query_id'] > 1]\n",
    "\n",
    "if not duplicate_pos_docs.empty:\n",
    "    print(f\"Found {len(duplicate_pos_docs)} positive documents that are used by multiple queries\")\n",
    "    \n",
    "    # # If you want to see the actual queries that share the same positive document\n",
    "    # for pos_doc_id in duplicate_pos_docs['pos_doc_id']:\n",
    "    #     queries_with_same_doc = triplets_df[triplets_df['pos_doc_id'] == pos_doc_id]\n",
    "    #     print(f\"\\nQueries sharing positive document {pos_doc_id}:\")\n",
    "    #     print(queries_with_same_doc[['query_id', 'pos_doc_id']])\n",
    "else:\n",
    "    print(\"No duplicate positive documents found across queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 83/83 [00:00<00:00, 2520.80ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 627/627 [00:00<00:00, 1155.24ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:18<00:00, 18.29s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 80/80 [00:00<00:00, 2701.32ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/amyf/ms-marco-triplets-train/commit/4a8b99603c7ca9dfd3d10a97a82655763c48b67b', commit_message='Upload dataset', commit_description='', oid='4a8b99603c7ca9dfd3d10a97a82655763c48b67b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/amyf/ms-marco-triplets-train', endpoint='https://huggingface.co', repo_type='dataset', repo_id='amyf/ms-marco-triplets-train'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Push to hugging face \n",
    "queries_dataset = Dataset.from_pandas(queries_df)\n",
    "documents_dataset = Dataset.from_pandas(documents_df)\n",
    "triplets_dataset = Dataset.from_pandas(triplets_df)\n",
    "\n",
    "queries_dataset.push_to_hub(\"amyf/ms-marco-queries-train\")\n",
    "documents_dataset.push_to_hub(\"amyf/ms-marco-documents-train\")\n",
    "triplets_dataset.push_to_hub(\"amyf/ms-marco-triplets-train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
